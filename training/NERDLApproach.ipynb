{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NERdl_approach.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHjCtuyPruYg",
        "colab_type": "text"
      },
      "source": [
        "# **Set up**\n",
        "\n",
        "Download the neccessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UMI8F6_r6CZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "8b7b4ce8-e116-40a7-dbb3-78851d071720"
      },
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed pyspark==2.4.4\n",
        "\n",
        "# Install Spark NLP\n",
        "! pip install --ignore-installed spark-nlp==2.5.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_252\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1~18.04-b09)\n",
            "OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)\n",
            "Processing /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471/pyspark-2.4.4-py2.py3-none-any.whl\n",
            "Collecting py4j==0.10.7\n",
            "  Using cached https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.4\n",
            "Collecting spark-nlp==2.5.1\n",
            "  Using cached https://files.pythonhosted.org/packages/df/b4/db653f8080a446de8ce981b262d85c85c61de7e920930726da0d1c6b4c65/spark_nlp-2.5.1-py2.py3-none-any.whl\n",
            "Installing collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQGC8qHTrevB",
        "colab_type": "text"
      },
      "source": [
        "# **Import Libraries**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW76xoyVsZ1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "\n",
        "import sparknlp\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.base import *\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG5OO-PttBls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start spark session\n",
        "spark = sparknlp.start()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE3Howdaujzi",
        "colab_type": "text"
      },
      "source": [
        "# **Download Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFFfSEzvCVfd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "66b685ed-ccdd-456b-b7a6-0bb388a43bcc"
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "\n",
        "urlretrieve('https://github.com/JohnSnowLabs/spark-nlp/raw/master/src/test/resources/conll2003/eng.train',\n",
        "           'eng.train')\n",
        "\n",
        "urlretrieve('https://github.com/JohnSnowLabs/spark-nlp/raw/master/src/test/resources/conll2003/eng.testa',\n",
        "           'eng.testa')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('eng.testa', <http.client.HTTPMessage at 0x7f4c86dd2828>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfOe0Mw0DnlV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "491b4f9e-5dce-4fb5-c088-67c7af63782f"
      },
      "source": [
        "from sparknlp.training import CoNLL\n",
        "\n",
        "#training_data_path = './eng.train'\n",
        "training_data_path = '/content/TestAddress.txt'\n",
        "#test_data_path = './eng.testa'\n",
        "\n",
        "training_data = CoNLL().readDataset(spark, training_data_path)\n",
        "training_data.show(3)\n",
        "\n",
        "#test_data = CoNLL().readDataset(spark, test_data_path)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|            sentence|               token|                 pos|               label|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|6557 ORLY DORVAL ...|[[document, 0, 37...|[[document, 0, 37...|[[token, 0, 3, 65...|[[pos, 0, 3, NNP,...|[[named_entity, 0...|\n",
            "|Level 14 BURJ DAM...|[[document, 0, 36...|[[document, 0, 36...|[[token, 0, 4, Le...|[[pos, 0, 4, NNP,...|[[named_entity, 0...|\n",
            "|HSBC Bank Middle ...|[[document, 0, 51...|[[document, 0, 51...|[[token, 0, 3, HS...|[[pos, 0, 3, NNP,...|[[named_entity, 0...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzCs6LW6EOyu",
        "colab_type": "text"
      },
      "source": [
        "# **Word embeddings**\n",
        "\n",
        " - Bert embeddings (look at BERT-as-a-service, essential feature extraction): different layers in BERT capture different information. `setPoolingLayer(0)` gives the first layer. That can be changed to anywhere between [-1,-12] depending on that information to capture. -1 will give information biased towards the training output, whereas -12 will give information close to the training input to the model, i.e. BERT adds close to no information in the embeddings.\n",
        " - Consider multilingual and ELMO embeddings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v98_ef-bEyqF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "6ee73f42-54dd-468e-8418-95202dde4106"
      },
      "source": [
        "bert_annotator = BertEmbeddings.pretrained('bert_base_cased', 'en') \\\n",
        " .setInputCols([\"sentence\",'token'])\\\n",
        " .setOutputCol(\"embeddings\")\\\n",
        " .setCaseSensitive(False)\\\n",
        " .setPoolingLayer(0)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_base_cased download started this may take some time.\n",
            "Approximate size to download 389.2 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfJ547G7KfS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_data = bert_annotator.transform(training_data)\n",
        "#test_data = bert_annotator.transform(test_data)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy-23h-P9GM-",
        "colab_type": "text"
      },
      "source": [
        "# **Train NER deep learning model**\n",
        "\n",
        " - `NerDLApproach()` trains Char CNNs - BiLSTM - CRF. ([Read more here.](https://https://arxiv.org/pdf/1603.01354.pdf))\n",
        " - Can experiment and build our own deep learning models in `tensorflow` and add the graph into spark-nlp lib. (Look more into how this is done.)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw1ORDrU9gSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nerTagger = NerDLApproach()\\\n",
        "  .setInputCols([\"sentence\", \"token\", 'embeddings'])\\\n",
        "  .setLabelColumn(\"label\")\\\n",
        "  .setOutputCol(\"ner\")\\\n",
        "  .setMaxEpochs(1)\\\n",
        "  .setLr(0.001)\\\n",
        "  .setPo(0.005)\\\n",
        "  .setBatchSize(8)\\\n",
        "  .setRandomSeed(0)\\\n",
        "  .setVerbose(1)\\\n",
        "  .setValidationSplit(0.2)\\\n",
        "  .setEvaluationLogExtended(True) \\\n",
        "  .setEnableOutputLogs(True)\\\n",
        "  .setIncludeConfidence(True)\n",
        "\n",
        "NER_pipeline = Pipeline(\n",
        "    stages = [\n",
        "    bert_annotator,\n",
        "    nerTagger\n",
        "  ])\n",
        "\n",
        "Ner_model = NER_pipeline.fit(training_data.limit(1000))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3hZ-26LXgEu",
        "colab_type": "text"
      },
      "source": [
        "# **Save the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSZ0vUaVXjzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_model = 'NER_model1'\n",
        "Ner_model.stages[1].write().overwrite().save(path_to_model)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sbFMfP3BBgN",
        "colab_type": "text"
      },
      "source": [
        "# **Prediction**\n",
        "\n",
        "1. On training data\n",
        "2. On test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hudpTAgRVV0P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "ec8583c5-5f97-42f0-d315-5436376d71bf"
      },
      "source": [
        "import pyspark.sql.functions as F\n",
        "predictions = Ner_model.transform(training_data)\n",
        "\n",
        "predictions.select(F.explode(F.arrays_zip('token.result','label.result','ner.result')).alias(\"cols\")) \\\n",
        ".select(F.expr(\"cols['0']\").alias(\"token\"),\n",
        "        F.expr(\"cols['1']\").alias(\"ground_truth\"),\n",
        "        F.expr(\"cols['2']\").alias(\"prediction\")).show(truncate=False)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+------------+----------+\n",
            "|token |ground_truth|prediction|\n",
            "+------+------------+----------+\n",
            "|6557  |B-House     |I-Street  |\n",
            "|ORLY  |B-Street    |O         |\n",
            "|DORVAL|I-Street    |I-Street  |\n",
            "|Quebec|B-State     |B-City    |\n",
            "|CANADA|B-Country   |I-Street  |\n",
            "|H9P   |B-Postcode  |I-Postcode|\n",
            "|1G1   |I-Postcode  |O         |\n",
            "|Level |B-House     |I-Street  |\n",
            "|14    |I-House     |O         |\n",
            "|BURJ  |B-Street    |I-Street  |\n",
            "|DAMAN |I-Street    |I-Street  |\n",
            "|DIFC  |I-Street    |I-Street  |\n",
            "|DUBAI |B-City      |O         |\n",
            "|UAE   |B-Country   |B-Country |\n",
            "|AE    |O           |O         |\n",
            "|HSBC  |O           |I-Postcode|\n",
            "|Bank  |O           |I-Street  |\n",
            "|Middle|O           |I-Street  |\n",
            "|East  |O           |I-Street  |\n",
            "|EMMAR |B-Street    |I-Street  |\n",
            "+------+------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSCWQiEAW5Lg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "5afb6685-6ecd-4898-ce50-2e8403a3e64d"
      },
      "source": [
        "document = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "sentence = SentenceDetector()\\\n",
        "    .setInputCols(['document'])\\\n",
        "    .setOutputCol('sentence')\n",
        "\n",
        "token = Tokenizer()\\\n",
        "    .setInputCols(['sentence'])\\\n",
        "    .setOutputCol('token')\n",
        "\n",
        "bert = BertEmbeddings.pretrained('bert_base_cased', 'en') \\\n",
        " .setInputCols([\"sentence\",'token'])\\\n",
        " .setOutputCol(\"embeddings\")\\\n",
        " .setCaseSensitive(False)\n",
        "\n",
        "loaded_ner_model = NerDLModel.load(path_to_model)\\\n",
        " .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
        " .setOutputCol(\"ner\")\n",
        "\n",
        "converter = NerConverter()\\\n",
        "  .setInputCols([\"document\", \"token\", \"ner\"])\\\n",
        "  .setOutputCol(\"ner_span\")\n",
        "\n",
        "ner_prediction_pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document,\n",
        "        sentence,\n",
        "        token,\n",
        "        bert,\n",
        "        loaded_ner_model,\n",
        "        converter])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_base_cased download started this may take some time.\n",
            "Approximate size to download 389.2 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDaUBYHaX-sd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "empty_data = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "prediction_model = ner_prediction_pipeline.fit(empty_data)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alSMW5k3Y2tB",
        "colab_type": "text"
      },
      "source": [
        "# **Test on new examples**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd4kq4gaY-0J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "c62ee8a7-cce6-4e26-ca54-dfea78c37d99"
      },
      "source": [
        "text = \"70 york street toronto ontario Canada l2n 8f4\"\n",
        "sample_data = spark.createDataFrame([[text]]).toDF(\"text\")\n",
        "sample_data.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|                text|\n",
            "+--------------------+\n",
            "|70 york street to...|\n",
            "+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvh0sZnqZIln",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "c11d5ebf-b777-4d1b-80db-8f3e38060114"
      },
      "source": [
        "import pyspark.sql.functions as F\n",
        "testpreds = prediction_model.transform(sample_data)\n",
        "testpreds.select(F.explode(F.arrays_zip('token.result','ner.result')).alias(\"cols\")) \\\n",
        ".select(F.expr(\"cols['0']\").alias(\"token\"),\n",
        "        F.expr(\"cols['1']\").alias(\"prediction\")).show(truncate=False)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------+\n",
            "|token  |prediction|\n",
            "+-------+----------+\n",
            "|70     |I-Street  |\n",
            "|york   |I-Street  |\n",
            "|street |I-Street  |\n",
            "|toronto|B-City    |\n",
            "|ontario|B-City    |\n",
            "|Canada |I-Street  |\n",
            "|l2n    |B-Postcode|\n",
            "|8f4    |I-Street  |\n",
            "+-------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
