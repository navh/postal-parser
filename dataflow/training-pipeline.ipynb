{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline Notebook\n",
    "Defines and deploys a training pipeline for transferring Open Addresses .csv files into CONLL .txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import apache_beam as beam\n",
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.runners import DataflowRunner\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "from datetime import timedelta\n",
    "\n",
    "import random\n",
    "import google.auth\n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install pyspark\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-controlled variables\n",
    "Change these variables for the data to be processed in your GCP setup\n",
    "\n",
    "Input data should always be within a single subfolder under the input location, with country folders within that.  Beyond that, files may be under state or province subfolders.  Each file should represent a single municipality, if the file is for a province or a country wide area, it should be named 'countrywide.csv', 'provincewide.csv', or 'statewide.csv'.\n",
    "\n",
    ".INPUT_LOCATION  \n",
    "&nbsp; &nbsp; +--subfolder  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; +--country1  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; +--state1  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; | &nbsp; +--city1.csv  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; | &nbsp; +--city2.csv  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; +--state2  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; | &nbsp; +--file1.csv  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; +-country2  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; +--province1  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; +--countrywide.csv  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; +-country3  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; +--countrywide.csv  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; |-...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'postal-parser-28'\n",
    "REGION = 'europe-west1'\n",
    "INPUT_LOCATION = \"gs://%s/test-unprocessed-data/testdata/**\" % BUCKET\n",
    "OUTPUT_LOCATION = \"gs://%s/parquet-sample\" % BUCKET\n",
    "PROBABILITY_SHUFFLE=0.2\n",
    "PROBABILITY_DELETE=0.2\n",
    "PROBABILITY_DUPLICATE=0.2\n",
    "\n",
    "# Labels as they appear in the files being loaded (should 1:1 map to HEADER)\n",
    "FILE_HEADER=['LON','LAT','NUMBER','STREET','UNIT','CITY','DISTRICT','REGION','POSTCODE','ID','HASH']\n",
    "# Labels you wish to be applied in the training data\n",
    "HEADER=['lon','lat','house_number','road','unit','city','state_district','state','postcode','id','hash']\n",
    "PIPELINE_OPTION_FLAGS = [\n",
    "    \"--requirements_file=requirements.txt\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCP Settings\n",
    "Sets up GCP settings, best to change settings in the above cell instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_training_location = \"%s/training/\" % OUTPUT_LOCATION\n",
    "output_testing_location = \"%s/testing/\" % OUTPUT_LOCATION\n",
    "dataflow_gcs_location = 'gs://%s/dataflow' % BUCKET\n",
    "options = pipeline_options.PipelineOptions(PIPELINE_OPTION_FLAGS)\n",
    "\n",
    "ib.options.capture_duration = timedelta(seconds=60)\n",
    "_, options.view_as(GoogleCloudOptions).project = google.auth.default()\n",
    "options.view_as(GoogleCloudOptions).region = REGION\n",
    "options.view_as(pipeline_options.SetupOptions).sdk_location = (\n",
    "    '/root/apache-beam-custom/packages/beam/sdks/python/dist/apache-beam-%s0.tar.gz' % \n",
    "    beam.version.__version__)\n",
    "options.view_as(GoogleCloudOptions).staging_location = '%s/staging' % dataflow_gcs_location\n",
    "options.view_as(GoogleCloudOptions).temp_location = '%s/temp' % dataflow_gcs_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Address Class\n",
    "Represents a single address and facilitates the transition from structured address to free-form string.  Also assigns proper NER tags to each entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddressFunc(beam.DoFn):\n",
    "    def init(self):\n",
    "        probability_shuffle=0.2 \n",
    "        probability_delete=0.2 \n",
    "        probability_duplicate=0.2\n",
    "        import random \n",
    "        \n",
    "        #import nltk\n",
    "        #nltk.download('averaged_perceptron_tagger')\n",
    "        \n",
    "        self.MAX_NUM_TAGS = 20\n",
    "        self._set_order()\n",
    "        if random.random() < probability_shuffle:\n",
    "            self._randomize_order()\n",
    "        self._duplicate_tags(probability_duplicate)\n",
    "        self._delete_tags(probability_delete)\n",
    "        self.ordered = False\n",
    "        \n",
    "    def _set_order(self):\n",
    "        import random\n",
    "        r = random.randint(0,2)\n",
    "        if r == 0:\n",
    "                new_order = ['house_number', 'road', 'city', 'city_district',\n",
    "                         'state_district', 'state', 'postcode', 'house', 'level', 'unit', 'po_box',\n",
    "                         'country']\n",
    "        elif r == 1:\n",
    "            new_order = ['house', 'house_number', 'po_box', 'road', 'city',\n",
    "                         'city_district', 'state_district', 'state', 'postcode', 'level', 'unit',\n",
    "                         'country']\n",
    "        else:\n",
    "            new_order = ['house', 'level', 'unit', 'po_box', 'house_number',\n",
    "                         'road', 'city', 'city_district',\n",
    "                         'state_district', 'state', 'postcode', 'country']\n",
    "        self.ordered = False\n",
    "        self.order = new_order\n",
    "\n",
    "    def _randomize_order(self):\n",
    "        import random\n",
    "        random.shuffle(self.order)\n",
    "        self.ordered = False\n",
    "\n",
    "    def _delete_tags(self, _delete_probability):\n",
    "        import random\n",
    "        while random.random() < _delete_probability and len(self.order) > 1:\n",
    "            del(self.order[random.randint(0, len(self.order)-1)])\n",
    "        self.ordered = False\n",
    "\n",
    "    def _duplicate_tags(self, _duplicate_probability):\n",
    "        import random\n",
    "        while random.random() < _duplicate_probability and len(self.order) < self.MAX_NUM_TAGS:\n",
    "            item_to_be_duplicated = self.order[random.randint(0, len(self.order)-1)]\n",
    "            self.order.insert(random.randint(0, len(self.order)), item_to_be_duplicated)\n",
    "        self.ordered = False\n",
    "    \n",
    "    def _remove_extra_labels(self):\n",
    "        # Description: Sorts csv_dict to create a list of dictionaries\n",
    "        #   such that they are in the same order they would be in an\n",
    "        #   address string written by a human.  Uses the order stored in class\n",
    "        i = 0\n",
    "        while i < len(self.order):\n",
    "            if not self.order[i] in self.address_dict:\n",
    "                del(self.order[i])\n",
    "            else:\n",
    "                i += 1\n",
    "        self.ordered = True\n",
    "\n",
    "    def _ner_tags(self):\n",
    "        tags = []\n",
    "        for header in self.order:\n",
    "            value = re.split('[ _]',self.address_dict[header])\n",
    "            tokens = []\n",
    "            tokens = tokens + [word for word in value if word]\n",
    "            for i in range(len(tokens)):\n",
    "                if i == 0:\n",
    "                    tags.append('B-' + header)\n",
    "                else:\n",
    "                    tags.append('I-' + header)\n",
    "        return tags\n",
    "\n",
    "    def _tokenize(self):\n",
    "        tokens = []\n",
    "        for header in self.order:\n",
    "            value = re.split('[ _]', self.address_dict[header])\n",
    "            tokens = tokens + [word for word in value if word]\n",
    "        return tokens\n",
    "    \n",
    "    def _to_string(self):\n",
    "        string_representation = ''\n",
    "        for k in self.order:\n",
    "            string_representation += self.address_dict[k] + ' '\n",
    "        string_representation = string_representation.strip()\n",
    "        return string_representation\n",
    "    \n",
    "    def _label(self):\n",
    "        if not self.ordered:\n",
    "            self._remove_extra_labels()\n",
    "        return {'string': self._to_string(), 'labels': self._ner_tags() ,'tokens': self._tokenize()}\n",
    "    \n",
    "    def process(self, element):\n",
    "        self.address_dict = element\n",
    "        self.init()\n",
    "        return [self._label()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Beam PTransform and DoFn classes\n",
    "Called throughout the pipeline execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileReader(beam.PTransform):\n",
    "    # Traverses GCP Storage and reads in any objects with prefix FILE_LOCATION and suffix '.csv'\n",
    "    # Flattens all file reads into a single PCollection\n",
    "    def __init__(self, FILE_LOCATION, pipeline_options):\n",
    "        self._FILE_LOCATION = FILE_LOCATION\n",
    "        self._options = pipeline_options\n",
    "    \n",
    "    def expand(self, pcoll):\n",
    "        from apache_beam.io.gcp.gcsfilesystem import GCSFileSystem\n",
    "        from apache_beam.io.filesystem import FileSystem\n",
    "        from apache_beam.io.filesystem import FileMetadata\n",
    "        from operator import add\n",
    "        from functools import reduce\n",
    "        \n",
    "        gcs = GCSFileSystem(self._options)\n",
    "        result = [m.metadata_list for m in gcs.match([self._FILE_LOCATION])]\n",
    "        result = reduce(add, result)\n",
    "        variables = ['p{}'.format(i) for i in range(len(result))]\n",
    "        read_labels = ['Read file {}'.format(i) for i in range(len(result))]\n",
    "        add_filename_labels = ['Add filename {}'.format(i) for i in range(len(result))]\n",
    "        \n",
    "        return (\n",
    "                [pcoll.pipeline \n",
    "                    | read_labels[i] >> beam.io.ReadFromText(result[i].path, skip_header_lines=1) \n",
    "                    | add_filename_labels[i] >> beam.ParDo(AddFilenamesFn(), result[i].path, self._FILE_LOCATION) \n",
    "                    for i in range(len(result)) if result[i].path.endswith('.csv')] \n",
    "                | 'Flatten PCollections' >> beam.Flatten() \n",
    "            )\n",
    "\n",
    "class AddFilenamesFn(beam.DoFn):\n",
    "    # ParDo to output a dict with filename and row\n",
    "    def process(self, element, file_path, base_path):\n",
    "        file_path_list = file_path.replace('.csv','').replace('_',' ').split(\"/\")[len(base_path.split('/'))-2:]\n",
    "        yield {'filename':file_path_list, 'row':element}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFormatting(beam.PTransform):\n",
    "    # Basic file formatting to extract data from files\n",
    "    def __init__(self, header):\n",
    "        self._header = header\n",
    "    def expand(self, pcoll):\n",
    "        return (\n",
    "            pcoll\n",
    "            | 'Parse CSV' >> beam.ParDo(ParseCSV())\n",
    "            | 'Build Dictionary' >> beam.ParDo(ToDict(), self._header)\n",
    "        )\n",
    "        \n",
    "class ParseCSV(beam.DoFn):    \n",
    "    # Parses out a line of text as a csv line\n",
    "    def process(self, element):\n",
    "        import csv\n",
    "        for line in csv.reader([element['row']], quotechar='\"', delimiter=',', quoting=csv.QUOTE_ALL, skipinitialspace=True):\n",
    "            return [{'filename':element['filename'], 'row':line}]\n",
    "        \n",
    "class ToDict(beam.DoFn):\n",
    "    # Breaks down a row to be a dictionary with header : entity\n",
    "    def process(self, element, header):\n",
    "        import re\n",
    "        FILE_NAME_FIELDS=['country','state']\n",
    "        line_dict = {}\n",
    "        for i in range(min(len(element['filename'])-2, 2)):\n",
    "            line_dict[FILE_NAME_FIELDS[i]] = element['filename'][i+1]\n",
    "        file_name = element['filename'][-1]\n",
    "        if not re.match('.*([0-9]|country|province|state|wide).*', file_name):\n",
    "            line_dict['city'] = file_name\n",
    "        \n",
    "        for i in range(min(len(header),len(element['row']))):\n",
    "            #TODO: Have entity headers be based on the header in the file rather than predefined\n",
    "            val = element['row'][i]\n",
    "            if val != '':\n",
    "                line_dict[header[i]] = element['row'][i]\n",
    "        return([line_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERFormatting(beam.PTransform):\n",
    "    #Formats the PCollection into a format to be taken in by the NER model\n",
    "    def expand(self, pcoll):\n",
    "        return (\n",
    "            pcoll\n",
    "            | 'To NER Inout' >> beam.ParDo(ToNER())\n",
    "            | 'Partition' >> beam.Partition(self.by_random, 2)\n",
    "        )\n",
    "    def by_random(self, element, num_partitions):\n",
    "        import random\n",
    "        PERCENT_TRAINING_DATA = 0.8\n",
    "        if random.random() < PERCENT_TRAINING_DATA:\n",
    "            return 0\n",
    "        return 1\n",
    "\n",
    "class ToNER(beam.DoFn):\n",
    "    #Changes the list of tokens and labels to the format needed for NER input for a given address\n",
    "    #element: {'tokens': [5, Main, St., ...], 'labels': [B-house_number, B-road, I-road]}\n",
    "    def process(self, element):\n",
    "        tokens, labels = element['tokens'], element['labels']\n",
    "        data=[]\n",
    "        lastBegin = 0\n",
    "        for i in range(len(tokens)):\n",
    "            a = {\n",
    "                'annotatorType' :  \"named_entity\",\n",
    "                'begin' : lastBegin,\n",
    "                'end' : lastBegin + len(tokens[i]) - 1,\n",
    "                'result' : labels[i],\n",
    "                'metadata' :  {'word': tokens[i]},\n",
    "                'embeddings' : [0.00]\n",
    "            }\n",
    "            lastBegin += len(tokens[i])+1\n",
    "            data.append(a)\n",
    "        return [{'string':element['text'], 'label':data}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Declaration\n",
    "Defines the path through which the pipeline executes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(InteractiveRunner())\n",
    "\n",
    "train, test = (\n",
    "    p \n",
    "    | 'Read files' >> FileReader(INPUT_LOCATION, options)\n",
    "    | 'Format Input' >> InputFormatting(HEADER)\n",
    "    | 'Order Adress' >> beam.ParDo(AddressFunc())\n",
    "    | 'To NER Input' >> NERFormatting()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs training split of data to training parquet\n",
    "_ = train | 'Write train parquet' >> beam.io.parquetio.WriteToParquet(\n",
    "        file_path_prefix=output_training_location,\n",
    "        schema=pyarrow.schema(\n",
    "            [('string',pyarrow.string()),\n",
    "             ('data',pyarrow.list_(\n",
    "                 pyarrow.struct([\n",
    "                    ('annotatorType', pyarrow.string()),\n",
    "                    ('begin', pyarrow.int32()),\n",
    "                    ('end', pyarrow.int32()),\n",
    "                    ('result', pyarrow.string()),\n",
    "                    ('metadata',pyarrow.struct([('word', pyarrow.string())])),\n",
    "                    ('embeddings', pyarrow.list_(pyarrow.float64()))\n",
    "                     ])))]\n",
    "        ),\n",
    "        file_name_suffix='.parquet')\n",
    "\n",
    "#Outputs testing split of data to testing parquet\n",
    "_ = test | 'Write test parquet' >> beam.io.parquetio.WriteToParquet(\n",
    "        file_path_prefix=output_testing_location,\n",
    "        schema=pyarrow.schema(\n",
    "            [('string',pyarrow.string()),\n",
    "             ('data',pyarrow.list_(\n",
    "                 pyarrow.struct([\n",
    "                    ('annotatorType', pyarrow.string()),\n",
    "                    ('begin', pyarrow.int32()),\n",
    "                    ('end', pyarrow.int32()),\n",
    "                    ('result', pyarrow.string()),\n",
    "                    ('metadata',pyarrow.struct([('word', pyarrow.string())])),\n",
    "                    ('embeddings', pyarrow.list_(pyarrow.float64()))\n",
    "                     ])))]\n",
    "        ),\n",
    "        file_name_suffix='.parquet')\n",
    "\n",
    "#Deprecated: for writing to text files instead of parquets\n",
    "#(train | 'Write training file' >> beam.io.WriteToText(output_training_location + '/conll.txt'))\n",
    "#(test | 'Write testing file' >> beam.io.WriteToText(output_testing_location + '/conll.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run pipeline on Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
       "            <div id=\"progress_indicator_2cbcb512284402132e7d6cd0b3c33747\" class=\"spinner-border text-info\" role=\"status\">\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.42.3 (20191010.1750)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"474pt\" height=\"548pt\"\n",
       " viewBox=\"0.00 0.00 473.50 548.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 544)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-544 469.5,-544 469.5,4 -4,4\"/>\n",
       "<!-- [143]: Read files -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>[143]: Read files</title>\n",
       "<polygon fill=\"none\" stroke=\"blue\" points=\"119.5,-540 11.5,-540 11.5,-504 119.5,-504 119.5,-540\"/>\n",
       "<text text-anchor=\"middle\" x=\"65.5\" y=\"-518.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"blue\">[143]: Read files</text>\n",
       "</g>\n",
       "<!-- pcoll9761 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>pcoll9761</title>\n",
       "<ellipse fill=\"none\" stroke=\"blue\" cx=\"65.5\" cy=\"-450\" rx=\"18\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- [143]: Read files&#45;&gt;pcoll9761 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>[143]: Read files&#45;&gt;pcoll9761</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M65.5,-503.7C65.5,-495.98 65.5,-486.71 65.5,-478.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"69,-478.1 65.5,-468.1 62,-478.1 69,-478.1\"/>\n",
       "</g>\n",
       "<!-- [143]: Format Input -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>[143]: Format Input</title>\n",
       "<polygon fill=\"none\" stroke=\"blue\" points=\"128,-396 3,-396 3,-360 128,-360 128,-396\"/>\n",
       "<text text-anchor=\"middle\" x=\"65.5\" y=\"-374.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"blue\">[143]: Format Input</text>\n",
       "</g>\n",
       "<!-- pcoll9761&#45;&gt;[143]: Format Input -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>pcoll9761&#45;&gt;[143]: Format Input</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M65.5,-431.7C65.5,-423.98 65.5,-414.71 65.5,-406.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"69,-406.1 65.5,-396.1 62,-406.1 69,-406.1\"/>\n",
       "</g>\n",
       "<!-- pcoll8561 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>pcoll8561</title>\n",
       "<ellipse fill=\"none\" stroke=\"blue\" cx=\"65.5\" cy=\"-306\" rx=\"18\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- [143]: Format Input&#45;&gt;pcoll8561 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>[143]: Format Input&#45;&gt;pcoll8561</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M65.5,-359.7C65.5,-351.98 65.5,-342.71 65.5,-334.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"69,-334.1 65.5,-324.1 62,-334.1 69,-334.1\"/>\n",
       "</g>\n",
       "<!-- [143]: Order Adress -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>[143]: Order Adress</title>\n",
       "<polygon fill=\"none\" stroke=\"blue\" points=\"128.5,-252 2.5,-252 2.5,-216 128.5,-216 128.5,-252\"/>\n",
       "<text text-anchor=\"middle\" x=\"65.5\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"blue\">[143]: Order Adress</text>\n",
       "</g>\n",
       "<!-- pcoll8561&#45;&gt;[143]: Order Adress -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>pcoll8561&#45;&gt;[143]: Order Adress</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M65.5,-287.7C65.5,-279.98 65.5,-270.71 65.5,-262.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"69,-262.1 65.5,-252.1 62,-262.1 69,-262.1\"/>\n",
       "</g>\n",
       "<!-- pcoll8488 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>pcoll8488</title>\n",
       "<ellipse fill=\"none\" stroke=\"blue\" cx=\"65.5\" cy=\"-162\" rx=\"18\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- [143]: Order Adress&#45;&gt;pcoll8488 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>[143]: Order Adress&#45;&gt;pcoll8488</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M65.5,-215.7C65.5,-207.98 65.5,-198.71 65.5,-190.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"69,-190.1 65.5,-180.1 62,-190.1 69,-190.1\"/>\n",
       "</g>\n",
       "<!-- [143]: To NER Input -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>[143]: To NER Input</title>\n",
       "<polygon fill=\"none\" stroke=\"blue\" points=\"131,-108 0,-108 0,-72 131,-72 131,-108\"/>\n",
       "<text text-anchor=\"middle\" x=\"65.5\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"blue\">[143]: To NER Input</text>\n",
       "</g>\n",
       "<!-- pcoll8488&#45;&gt;[143]: To NER Input -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>pcoll8488&#45;&gt;[143]: To NER Input</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M65.5,-143.7C65.5,-135.98 65.5,-126.71 65.5,-118.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"69,-118.1 65.5,-108.1 62,-118.1 69,-118.1\"/>\n",
       "</g>\n",
       "<!-- pcoll5399 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>pcoll5399</title>\n",
       "<ellipse fill=\"none\" stroke=\"blue\" cx=\"65.5\" cy=\"-18\" rx=\"18\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- [143]: To NER Input&#45;&gt;pcoll5399 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>[143]: To NER Input&#45;&gt;pcoll5399</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M65.5,-71.7C65.5,-63.98 65.5,-54.71 65.5,-46.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"69,-46.1 65.5,-36.1 62,-46.1 69,-46.1\"/>\n",
       "</g>\n",
       "<!-- [144]: Write train parquet -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>[144]: Write train parquet</title>\n",
       "<polygon fill=\"none\" stroke=\"blue\" points=\"295.5,-540 137.5,-540 137.5,-504 295.5,-504 295.5,-540\"/>\n",
       "<text text-anchor=\"middle\" x=\"216.5\" y=\"-518.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"blue\">[144]: Write train parquet</text>\n",
       "</g>\n",
       "<!-- pcoll5791 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>pcoll5791</title>\n",
       "<ellipse fill=\"none\" stroke=\"blue\" cx=\"216.5\" cy=\"-450\" rx=\"18\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- [144]: Write train parquet&#45;&gt;pcoll5791 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>[144]: Write train parquet&#45;&gt;pcoll5791</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M216.5,-503.7C216.5,-495.98 216.5,-486.71 216.5,-478.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"220,-478.1 216.5,-468.1 213,-478.1 220,-478.1\"/>\n",
       "</g>\n",
       "<!-- [144]: Write test parquet -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>[144]: Write test parquet</title>\n",
       "<polygon fill=\"none\" stroke=\"blue\" points=\"465.5,-540 313.5,-540 313.5,-504 465.5,-504 465.5,-540\"/>\n",
       "<text text-anchor=\"middle\" x=\"389.5\" y=\"-518.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"blue\">[144]: Write test parquet</text>\n",
       "</g>\n",
       "<!-- _ -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>_</title>\n",
       "<ellipse fill=\"none\" stroke=\"blue\" cx=\"389.5\" cy=\"-450\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"389.5\" y=\"-446.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"blue\">_</text>\n",
       "</g>\n",
       "<!-- [144]: Write test parquet&#45;&gt;_ -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>[144]: Write test parquet&#45;&gt;_</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M389.5,-503.7C389.5,-495.98 389.5,-486.71 389.5,-478.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"393,-478.1 389.5,-468.1 386,-478.1 393,-478.1\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "            $(\"#progress_indicator_2cbcb512284402132e7d6cd0b3c33747\").remove();\n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "            $(\"#progress_indicator_2cbcb512284402132e7d6cd0b3c33747\").remove();\n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ib.show_graph(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n"
     ]
    }
   ],
   "source": [
    "pipeline_result = DataflowRunner().run_pipeline(p, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Click <a href=\"https://console.cloud.google.com/dataflow/jobs/europe-west1/2020-08-11_09_57_44-11042933841220375304?project=postal-parser-28\" target=\"_new\">here</a> for the details of your Dataflow job!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "url = ('https://console.cloud.google.com/dataflow/jobs/%s/%s?project=%s' % \n",
    "      (pipeline_result._job.location, pipeline_result._job.id, pipeline_result._job.projectId))\n",
    "display(HTML('Click <a href=\"%s\" target=\"_new\">here</a> for the details of your Dataflow job!' % url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show PCollection for testing purposes\n",
    "Runs locally, don't run this on large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ib.show(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Beam 2.24.0.dev0 for Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
