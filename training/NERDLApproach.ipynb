{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VQGC8qHTrevB"
   },
   "source": [
    "# **Import Libraries**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dW76xoyVsZ1S"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fG5OO-PttBls"
   },
   "outputs": [],
   "source": [
    "# Start spark session\n",
    "spark = sparknlp.start()\n",
    "def start(gpu):\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(\"Spark NLP\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"8G\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"1000M\")\n",
    "    if gpu:\n",
    "        builder.config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.5.1\")\n",
    "    else:\n",
    "        builder.config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.1\")\n",
    "\n",
    "    return builder.getOrCreate()\n",
    "\n",
    "gpu_access=False  \n",
    "spark = start(gpu=gpu_access)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE3Howdaujzi"
   },
   "source": [
    "# **Read Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "FfOe0Mw0DnlV",
    "outputId": "491b4f9e-5dce-4fb5-c088-67c7af63782f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|6557 ORLY DORVAL ...|[[document, 0, 37...|[[document, 0, 37...|[[token, 0, 3, 65...|[[pos, 0, 3, NNP,...|[[named_entity, 0...|\n",
      "|Level 14 BURJ DAM...|[[document, 0, 36...|[[document, 0, 36...|[[token, 0, 4, Le...|[[pos, 0, 4, NNP,...|[[named_entity, 0...|\n",
      "|HSBC Bank Middle ...|[[document, 0, 51...|[[document, 0, 51...|[[token, 0, 3, HS...|[[pos, 0, 3, NNP,...|[[named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.training import CoNLL\n",
    "\n",
    "\n",
    "training_data_path = '../data/CoNLL_addresses.txt'\n",
    "\n",
    "\n",
    "training_data = CoNLL().readDataset(spark, training_data_path)\n",
    "training_data.show(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VzCs6LW6EOyu"
   },
   "source": [
    "# **Word embeddings**\n",
    "\n",
    " - Bert embeddings (look at BERT-as-a-service, essential feature extraction): different layers in BERT capture different information. `setPoolingLayer(0)` gives the first layer. That can be changed to anywhere between [-1,-12] depending on that information to capture. -1 will give information biased towards the training output, whereas -12 will give information close to the training input to the model, i.e. BERT adds close to no information in the embeddings.\n",
    " - Consider multilingual and ELMO embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "v98_ef-bEyqF",
    "outputId": "6ee73f42-54dd-468e-8418-95202dde4106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_base_cased download started this may take some time.\n",
      "Approximate size to download 389.2 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "bert_annotator = BertEmbeddings.pretrained('bert_base_cased', 'en') \\\n",
    " .setInputCols([\"sentence\",'token'])\\\n",
    " .setOutputCol(\"embeddings\")\\\n",
    " .setCaseSensitive(False)\\\n",
    " .setPoolingLayer(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bfJ547G7KfS7"
   },
   "outputs": [],
   "source": [
    "training_data = bert_annotator.transform(training_data)\n",
    "#test_data = bert_annotator.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qy-23h-P9GM-"
   },
   "source": [
    "# **Train NER deep learning model**\n",
    "\n",
    " - `NerDLApproach()` trains Char CNNs - BiLSTM - CRF. ([Read more here.](https://https://arxiv.org/pdf/1603.01354.pdf))\n",
    " - Can experiment and build our own deep learning models in `tensorflow` and add the graph into spark-nlp lib. (Look more into how this is done.)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rw1ORDrU9gSG"
   },
   "outputs": [],
   "source": [
    "nerTagger = NerDLApproach()\\\n",
    "  .setInputCols([\"sentence\", \"token\", \"bert\"])\\\n",
    "  .setLabelColumn(\"label\")\\\n",
    "  .setOutputCol(\"ner\")\\\n",
    "  .setMaxEpochs(1)\\\n",
    "  .setLr(0.001)\\\n",
    "  .setPo(0.005)\\\n",
    "  .setBatchSize(8)\\\n",
    "  .setRandomSeed(0)\\\n",
    "  .setVerbose(1)\\\n",
    "  .setValidationSplit(0.2)\\\n",
    "  .setEvaluationLogExtended(True) \\\n",
    "  .setEnableOutputLogs(True)\\\n",
    "  .setIncludeConfidence(True)\\\n",
    "  .setGraphFolder(\"graph\")\n",
    "\n",
    "NER_pipeline = Pipeline(\n",
    "    stages = [\n",
    "    bert_annotator,\n",
    "    nerTagger\n",
    "  ])\n",
    "\n",
    "Ner_model = NER_pipeline.fit(training_data.limit(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S3hZ-26LXgEu"
   },
   "source": [
    "# **Save the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RSZ0vUaVXjzl"
   },
   "outputs": [],
   "source": [
    "path_to_model = 'NER_model1'\n",
    "Ner_model.stages[1].write().overwrite().save(path_to_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8sbFMfP3BBgN"
   },
   "source": [
    "# **Prediction**\n",
    "\n",
    "1. On training data\n",
    "2. On test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "colab_type": "code",
    "id": "hudpTAgRVV0P",
    "outputId": "ec8583c5-5f97-42f0-d315-5436376d71bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+\n",
      "|token |ground_truth|prediction|\n",
      "+------+------------+----------+\n",
      "|6557  |B-House     |I-Street  |\n",
      "|ORLY  |B-Street    |O         |\n",
      "|DORVAL|I-Street    |I-Street  |\n",
      "|Quebec|B-State     |B-City    |\n",
      "|CANADA|B-Country   |I-Street  |\n",
      "|H9P   |B-Postcode  |I-Postcode|\n",
      "|1G1   |I-Postcode  |O         |\n",
      "|Level |B-House     |I-Street  |\n",
      "|14    |I-House     |O         |\n",
      "|BURJ  |B-Street    |I-Street  |\n",
      "|DAMAN |I-Street    |I-Street  |\n",
      "|DIFC  |I-Street    |I-Street  |\n",
      "|DUBAI |B-City      |O         |\n",
      "|UAE   |B-Country   |B-Country |\n",
      "|AE    |O           |O         |\n",
      "|HSBC  |O           |I-Postcode|\n",
      "|Bank  |O           |I-Street  |\n",
      "|Middle|O           |I-Street  |\n",
      "|East  |O           |I-Street  |\n",
      "|EMMAR |B-Street    |I-Street  |\n",
      "+------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "predictions = Ner_model.transform(training_data)\n",
    "\n",
    "predictions.select(F.explode(F.arrays_zip('token.result','label.result','ner.result')).alias(\"cols\")) \\\n",
    ".select(F.expr(\"cols['0']\").alias(\"token\"),\n",
    "        F.expr(\"cols['1']\").alias(\"ground_truth\"),\n",
    "        F.expr(\"cols['2']\").alias(\"prediction\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "NSCWQiEAW5Lg",
    "outputId": "5afb6685-6ecd-4898-ce50-2e8403a3e64d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_base_cased download started this may take some time.\n",
      "Approximate size to download 389.2 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence = SentenceDetector()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentence')\n",
    "\n",
    "token = Tokenizer()\\\n",
    "    .setInputCols(['sentence'])\\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "bert = BertEmbeddings.pretrained('bert_base_cased', 'en') \\\n",
    " .setInputCols([\"sentence\",'token'])\\\n",
    " .setOutputCol(\"embeddings\")\\\n",
    " .setCaseSensitive(False)\n",
    "\n",
    "loaded_ner_model = NerDLModel.load(path_to_model)\\\n",
    " .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
    " .setOutputCol(\"ner\")\n",
    "\n",
    "converter = NerConverter()\\\n",
    "  .setInputCols([\"document\", \"token\", \"ner\"])\\\n",
    "  .setOutputCol(\"ner_span\")\n",
    "\n",
    "ner_prediction_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        sentence,\n",
    "        token,\n",
    "        bert,\n",
    "        loaded_ner_model,\n",
    "        converter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BDaUBYHaX-sd"
   },
   "outputs": [],
   "source": [
    "empty_data = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "prediction_model = ner_prediction_pipeline.fit(empty_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "alSMW5k3Y2tB"
   },
   "source": [
    "# **Test on new examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "wd4kq4gaY-0J",
    "outputId": "c62ee8a7-cce6-4e26-ca54-dfea78c37d99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|70 york street to...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"70 york street toronto ontario Canada l2n 8f4\"\n",
    "sample_data = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "sample_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "nvh0sZnqZIln",
    "outputId": "c11d5ebf-b777-4d1b-80db-8f3e38060114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|token  |prediction|\n",
      "+-------+----------+\n",
      "|70     |I-Street  |\n",
      "|york   |I-Street  |\n",
      "|street |I-Street  |\n",
      "|toronto|B-City    |\n",
      "|ontario|B-City    |\n",
      "|Canada |I-Street  |\n",
      "|l2n    |B-Postcode|\n",
      "|8f4    |I-Street  |\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "testpreds = prediction_model.transform(sample_data)\n",
    "testpreds.select(F.explode(F.arrays_zip('token.result','ner.result')).alias(\"cols\")) \\\n",
    ".select(F.expr(\"cols['0']\").alias(\"token\"),\n",
    "        F.expr(\"cols['1']\").alias(\"prediction\")).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NERdl_approach.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
