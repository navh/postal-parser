{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline Notebook\n",
    "Defines and deploys a preprocessing pipeline for transferring Open Addresses .csv files into parquet files formatted for NER input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import apache_beam as beam\n",
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.runners import DataflowRunner\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "from datetime import timedelta\n",
    "\n",
    "import random\n",
    "import google.auth\n",
    "import pyarrow\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install pyspark\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-controlled variables\n",
    "Change these variables for the data to be processed in your GCP setup\n",
    "\n",
    "Input data should always be within a single subfolder under the input location, with country folders within that.  Beyond that, files may be under state or province subfolders.  Each file should represent a single municipality, if the file is for a province or a country wide area, it should be named 'countrywide.csv', 'provincewide.csv', or 'statewide.csv'.\n",
    "\n",
    ".INPUT_LOCATION  \n",
    "&nbsp; &nbsp; +--subfolder  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; +--country1  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; +--state1  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; | &nbsp; +--city1.csv  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; | &nbsp; +--city2.csv  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; +--state2  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; | &nbsp; +--file1.csv  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; +-country2  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; +--province1  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; +--countrywide.csv  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; +-country3  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; +--countrywide.csv  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; |-...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bucket ID\n",
    "BUCKET = 'postal-parser-28'\n",
    "#If using Maplequad, should be europe-west1 or europe-west2\n",
    "REGION = 'europe-west1'\n",
    "INPUT_LOCATION = \"gs://%s/unprocessed-data/**\" % BUCKET\n",
    "OUTPUT_LOCATION = \"gs://%s/processed-data\" % BUCKET\n",
    "\n",
    "#Data randomization probabilities. \n",
    "PROBABILITY_SHUFFLE=0.2\n",
    "PROBABILITY_DELETE=0.2\n",
    "PROBABILITY_DUPLICATE=0.2\n",
    "\n",
    "# Labels as they appear in the files being loaded (should 1:1 map to HEADER)\n",
    "FILE_HEADER=['LON','LAT','NUMBER','STREET','UNIT','CITY','DISTRICT','REGION','POSTCODE','ID','HASH']\n",
    "# Labels you wish to be applied in the training data\n",
    "HEADER=['lon','lat','house_number','road','unit','city','state_district','state','postcode','id','hash']\n",
    "PIPELINE_OPTION_FLAGS = [\n",
    "    \"--requirements_file=requirements.txt\"\n",
    "]\n",
    "# If running on GCP VM, keep this as is, otherwise you can customize it to your file l\n",
    "APACHE_BEAM_LOCATION = (\n",
    "    '/root/apache-beam-custom/packages/beam/sdks/python/dist/apache-beam-%s0.tar.gz' % \n",
    "    beam.version.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCP Settings\n",
    "Sets up GCP settings, best to change settings in the above cell instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_training_location = \"%s/training/part\" % OUTPUT_LOCATION\n",
    "output_testing_location = \"%s/testing/part\" % OUTPUT_LOCATION\n",
    "dataflow_gcs_location = 'gs://%s/dataflow' % BUCKET\n",
    "options = pipeline_options.PipelineOptions(PIPELINE_OPTION_FLAGS)\n",
    "\n",
    "ib.options.capture_duration = timedelta(seconds=60)\n",
    "_, options.view_as(GoogleCloudOptions).project = google.auth.default()\n",
    "options.view_as(GoogleCloudOptions).region = REGION\n",
    "options.view_as(pipeline_options.SetupOptions).sdk_location = APACHE_BEAM_LOCATION\n",
    "options.view_as(GoogleCloudOptions).staging_location = '%s/staging' % dataflow_gcs_location\n",
    "options.view_as(GoogleCloudOptions).temp_location = '%s/temp' % dataflow_gcs_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Address Class\n",
    "Represents a single address and facilitates the transition from structured address to free-form string.  Also assigns proper NER tags to each entity. Executed after first 2 Apache Beam PTransforms.\n",
    "\n",
    "#### ParDo(AddressFunc()) Output PCollection:  \n",
    "| 'text'                  | 'labels'                                                | 'tokens'                          |   |   |\n",
    "|-------------------------|---------------------------------------------------------|-----------------------------------|---|---|\n",
    "| '70 York St Toronto ON' | ['B-house_number','B-street','I-street','city','state'] | ['70','York','St','Toronto','ON'] |   |   |\n",
    "| ...                     | ...                                                     | ...                               |   |   |\n",
    "|                         |                                                         |                                   |   |   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddressFunc(beam.DoFn):\n",
    "    def init(self):\n",
    "        probability_shuffle=0.2 \n",
    "        probability_delete=0.2 \n",
    "        probability_duplicate=0.2\n",
    "        import random \n",
    "        \n",
    "        #import nltk\n",
    "        #nltk.download('averaged_perceptron_tagger')\n",
    "        \n",
    "        self.MAX_NUM_TAGS = 20\n",
    "        self._set_order()\n",
    "        if random.random() < probability_shuffle:\n",
    "            self._randomize_order()\n",
    "        self._duplicate_tags(probability_duplicate)\n",
    "        self._delete_tags(probability_delete)\n",
    "        self.ordered = False\n",
    "        \n",
    "    def _set_order(self):\n",
    "        import random\n",
    "        r = random.randint(0,2)\n",
    "        if r == 0:\n",
    "                new_order = ['house_number', 'road', 'city', 'city_district',\n",
    "                         'state_district', 'state', 'postcode', 'house', 'level', 'unit', 'po_box',\n",
    "                         'country']\n",
    "        elif r == 1:\n",
    "            new_order = ['house', 'house_number', 'po_box', 'road', 'city',\n",
    "                         'city_district', 'state_district', 'state', 'postcode', 'level', 'unit',\n",
    "                         'country']\n",
    "        else:\n",
    "            new_order = ['house', 'level', 'unit', 'po_box', 'house_number',\n",
    "                         'road', 'city', 'city_district',\n",
    "                         'state_district', 'state', 'postcode', 'country']\n",
    "        self.ordered = False\n",
    "        self.order = new_order\n",
    "\n",
    "    def _randomize_order(self):\n",
    "        import random\n",
    "        random.shuffle(self.order)\n",
    "        self.ordered = False\n",
    "\n",
    "    def _delete_tags(self, _delete_probability):\n",
    "        import random\n",
    "        while random.random() < _delete_probability and len(self.order) > 1:\n",
    "            del(self.order[random.randint(0, len(self.order)-1)])\n",
    "        self.ordered = False\n",
    "\n",
    "    def _duplicate_tags(self, _duplicate_probability):\n",
    "        import random\n",
    "        while random.random() < _duplicate_probability and len(self.order) < self.MAX_NUM_TAGS:\n",
    "            item_to_be_duplicated = self.order[random.randint(0, len(self.order)-1)]\n",
    "            self.order.insert(random.randint(0, len(self.order)), item_to_be_duplicated)\n",
    "        self.ordered = False\n",
    "    \n",
    "    def _remove_extra_labels(self):\n",
    "        # Description: Sorts csv_dict to create a list of dictionaries\n",
    "        #   such that they are in the same order they would be in an\n",
    "        #   address string written by a human.  Uses the order stored in class\n",
    "        i = 0\n",
    "        while i < len(self.order):\n",
    "            if not self.order[i] in self.address_dict:\n",
    "                del(self.order[i])\n",
    "            else:\n",
    "                i += 1\n",
    "        self.ordered = True\n",
    "\n",
    "    def _ner_tags(self):\n",
    "        tags = []\n",
    "        for header in self.order:\n",
    "            value = re.split('[ _]',self.address_dict[header])\n",
    "            tokens = []\n",
    "            tokens = tokens + [word for word in value if word]\n",
    "            for i in range(len(tokens)):\n",
    "                if i == 0:\n",
    "                    tags.append('B-' + header)\n",
    "                else:\n",
    "                    tags.append('I-' + header)\n",
    "        return tags\n",
    "\n",
    "    def _tokenize(self):\n",
    "        tokens = []\n",
    "        for header in self.order:\n",
    "            value = re.split('[ _]', self.address_dict[header])\n",
    "            tokens = tokens + [word for word in value if word]\n",
    "        return tokens\n",
    "    \n",
    "    def _to_string(self):\n",
    "        string_representation = ''\n",
    "        for k in self.order:\n",
    "            string_representation += self.address_dict[k] + ' '\n",
    "        string_representation = string_representation.strip()\n",
    "        return string_representation\n",
    "    \n",
    "    def _label(self):\n",
    "        if not self.ordered:\n",
    "            self._remove_extra_labels()\n",
    "        return {'text': self._to_string(), 'labels': self._ner_tags() ,'tokens': self._tokenize()}\n",
    "    \n",
    "    def process(self, element):\n",
    "        self.address_dict = element\n",
    "        self.init()\n",
    "        return [self._label()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Beam PTransform and DoFn classes\n",
    "Called throughout the pipeline execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FileReader Output PCollection\n",
    "| 'filename'            | 'row'                                                        |   |   |   |\n",
    "|-----------------------|--------------------------------------------------------------|---|---|---|\n",
    "| ['ca','on','Toronto'] | '11.521,47.2313,70,York St,,Toronto,,,M5H 1J8,,some_hash_id' |   |   |   |\n",
    "| ...                   | ...                                                          |   |   |   |\n",
    "|                       |                                                              |   |   |   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileReader(beam.PTransform):\n",
    "    # Traverses GCP Storage and reads in any objects with prefix FILE_LOCATION and suffix '.csv'\n",
    "    # Flattens all file reads into a single PCollection\n",
    "    def __init__(self, FILE_LOCATION, pipeline_options):\n",
    "        self._FILE_LOCATION = FILE_LOCATION\n",
    "        self._options = pipeline_options\n",
    "    \n",
    "    def expand(self, pcoll):\n",
    "        from apache_beam.io.gcp.gcsfilesystem import GCSFileSystem\n",
    "        from apache_beam.io.filesystem import FileSystem\n",
    "        from apache_beam.io.filesystem import FileMetadata\n",
    "        from operator import add\n",
    "        from functools import reduce\n",
    "        import re\n",
    "        \n",
    "        gcs = GCSFileSystem(self._options)\n",
    "        result = [m.metadata_list for m in gcs.match([self._FILE_LOCATION])]\n",
    "        result = reduce(add, result)\n",
    "        variables = ['p{}'.format(i) for i in range(len(result))]\n",
    "        read_labels = ['Read file {}'.format(i) for i in range(len(result))]\n",
    "        add_filename_labels = ['Add filename {}'.format(i) for i in range(len(result))]\n",
    "        \n",
    "        return (\n",
    "                [pcoll.pipeline \n",
    "                    | read_labels[i] >> beam.io.ReadFromText(result[i].path, skip_header_lines=1) \n",
    "                    | add_filename_labels[i] >> beam.ParDo(AddFilenamesFn(), result[i].path, self._FILE_LOCATION) \n",
    "                    for i in range(len(result)) if result[i].path.endswith('.csv')]\n",
    "                | 'Flatten PCollections' >> beam.Flatten() \n",
    "            )\n",
    "\n",
    "class AddFilenamesFn(beam.DoFn):\n",
    "    # ParDo to output a dict with filename and row\n",
    "    def process(self, element, file_path, base_path):\n",
    "        file_path_list = file_path.replace('.csv','').replace('_',' ').split(\"/\")[len(base_path.split('/'))-1:]\n",
    "        yield {'filename':file_path_list, 'row':element}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InputFormatting Output PCollection (Stored as collection of dictionaries)\n",
    "| 'lat'    | 'lon'     | 'house_number' | 'road'    | 'state' | 'city'    | 'postcode' | ... |\n",
    "|----------|-----------|----------------|-----------|---------|-----------|------------|-----|\n",
    "| '11.521' | '47.2313' | '70'           | 'York St' | 'on'    | 'Toronto' | 'M5H 1J8'  | ... |\n",
    "| ...      | ...       | ...            | ...       | ...     | ...       | ...        | ... |\n",
    "|          |           |                |           |         |           |            |     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFormatting(beam.PTransform):\n",
    "    # Basic file formatting to extract data from files\n",
    "    def __init__(self, header):\n",
    "        self._header = header\n",
    "    def expand(self, pcoll):\n",
    "        return (\n",
    "            pcoll\n",
    "            | 'Parse CSV' >> beam.ParDo(ParseCSV())\n",
    "            | 'Build Dictionary' >> beam.ParDo(ToDict(), self._header)\n",
    "        )\n",
    "        \n",
    "class ParseCSV(beam.DoFn):    \n",
    "    # Parses out a line of text as a csv line\n",
    "    def process(self, element):\n",
    "        import csv\n",
    "        for line in csv.reader([element['row'].replace('.','')], quotechar='\"', delimiter=',', quoting=csv.QUOTE_ALL, skipinitialspace=True):\n",
    "            return [{'filename':element['filename'], 'row':line}]\n",
    "        \n",
    "class ToDict(beam.DoFn):\n",
    "    # Breaks down a row to be a dictionary with header : entity\n",
    "    def process(self, element, header):\n",
    "        import re\n",
    "        \n",
    "        #add info from the filename to the dictionary\n",
    "        FILE_NAME_FIELDS=['country','state']\n",
    "        line_dict = {}\n",
    "        for i in range(min(len(element['filename'])-2, 2)):\n",
    "            line_dict[FILE_NAME_FIELDS[i]] = element['filename'][i+1]\n",
    "        file_name = element['filename'][-1]\n",
    "        if not re.match('.*([0-9]|country|province|state|wide).*', file_name):\n",
    "            line_dict['city'] = file_name\n",
    "        \n",
    "        #insert the info from the row itself into the dictionary\n",
    "        for i in range(min(len(header),len(element['row']))):\n",
    "            #TODO: Have entity headers be based on the header in the file rather than predefined\n",
    "            val = element['row'][i]\n",
    "            if val != '':\n",
    "                line_dict[header[i]] = element['row'][i]\n",
    "        return([line_dict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NERFormatting Output PCollection\n",
    "| 'text'                  | 'label'                                                                                                                        |   |   |   |\n",
    "|-------------------------|--------------------------------------------------------------------------------------------------------------------------------|---|---|---|\n",
    "| '70 York St Toronto ON' | [{'annotatorType':'named_entity','begin':0,'end':1,'result':'B-house_number','metadata':{'word':'70'},'embeddings':[0.0]},...] |   |   |   |\n",
    "| ...                     | ...                                                                                                                            |   |   |   |\n",
    "|                         |                                                                                                                                |   |   |   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERFormatting(beam.PTransform):\n",
    "    #Formats the PCollection into a format to be taken in by the NER model\n",
    "    def expand(self, pcoll):\n",
    "        return (\n",
    "            pcoll\n",
    "            | 'To NER Inout' >> beam.ParDo(ToNER())\n",
    "            | 'Partition' >> beam.Partition(self.by_random, 2)\n",
    "        )\n",
    "    def by_random(self, element, num_partitions):\n",
    "        import random\n",
    "        PERCENT_TRAINING_DATA = 0.8\n",
    "        if random.random() < PERCENT_TRAINING_DATA:\n",
    "            return 0\n",
    "        return 1\n",
    "\n",
    "class ToNER(beam.DoFn):\n",
    "    #Changes the list of tokens and labels to the format needed for NER input for a given address\n",
    "    #element: {'tokens': [5, Main, St., ...], 'labels': [B-house_number, B-road, I-road]}\n",
    "    def process(self, element):\n",
    "        tokens, labels = element['tokens'], element['labels']\n",
    "        data=[]\n",
    "        lastBegin = 0\n",
    "        for i in range(len(tokens)):\n",
    "            a = {\n",
    "                'annotatorType' :  \"named_entity\",\n",
    "                'begin' : lastBegin,\n",
    "                'end' : lastBegin + len(tokens[i]) - 1,\n",
    "                'result' : labels[i],\n",
    "                'metadata' :  {'word': tokens[i]},\n",
    "                'embeddings' : [0.00]\n",
    "            }\n",
    "            lastBegin += len(tokens[i])+1\n",
    "            data.append(a)\n",
    "        return [{'text':element['text'], 'label':data}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Declaration\n",
    "Defines the path through which the pipeline executes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(InteractiveRunner())\n",
    "\n",
    "train, test = (\n",
    "    p \n",
    "    | 'Read files' >> FileReader(INPUT_LOCATION, options)\n",
    "    | 'Format Input' >> InputFormatting(HEADER)\n",
    "    | 'Order Adress' >> beam.ParDo(AddressFunc())\n",
    "    | 'To NER Input' >> NERFormatting()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs training split of data to training parquet\n",
    "_ = train | 'Write train parquet' >> beam.io.parquetio.WriteToParquet(\n",
    "        file_path_prefix=output_training_location,\n",
    "        schema=pyarrow.schema(\n",
    "            [('text',pyarrow.string()),\n",
    "             ('label',pyarrow.list_(\n",
    "                 pyarrow.struct([\n",
    "                    ('annotatorType', pyarrow.string()),\n",
    "                    ('begin', pyarrow.int32()),\n",
    "                    ('end', pyarrow.int32()),\n",
    "                    ('result', pyarrow.string()),\n",
    "                    ('metadata',pyarrow.struct([('word', pyarrow.string())])),\n",
    "                    ('embeddings', pyarrow.list_(pyarrow.float64()))\n",
    "                     ])))]\n",
    "        ),\n",
    "        file_name_suffix='.parquet')\n",
    "\n",
    "#Outputs testing split of data to testing parquet\n",
    "_ = test | 'Write test parquet' >> beam.io.parquetio.WriteToParquet(\n",
    "        file_path_prefix=output_testing_location,\n",
    "        schema=pyarrow.schema(\n",
    "            [('text',pyarrow.string()),\n",
    "             ('label',pyarrow.list_(\n",
    "                 pyarrow.struct([\n",
    "                    ('annotatorType', pyarrow.string()),\n",
    "                    ('begin', pyarrow.int32()),\n",
    "                    ('end', pyarrow.int32()),\n",
    "                    ('result', pyarrow.string()),\n",
    "                    ('metadata',pyarrow.struct([('word', pyarrow.string())])),\n",
    "                    ('embeddings', pyarrow.list_(pyarrow.float64()))\n",
    "                     ])))]\n",
    "        ),\n",
    "        file_name_suffix='.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run pipeline on Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show_graph(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result = DataflowRunner().run_pipeline(p, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "url = ('https://console.cloud.google.com/dataflow/jobs/%s/%s?project=%s' % \n",
    "      (pipeline_result._job.location, pipeline_result._job.id, pipeline_result._job.projectId))\n",
    "display(HTML('Click <a href=\"%s\" target=\"_new\">here</a> for the details of your Dataflow job!' % url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show PCollection for testing purposes\n",
    "Runs locally, don't run this on large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ib.show(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Beam 2.24.0.dev0 for Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
